{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Session ile PySpark kodumuzu Spark kümesine bağlar. Kodun çalışması için gerekli olan tüm yapılandırmaları oluşturur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Data_Wrangling\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Spark version: 3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Print PySpark and Python versions\n",
    "import sys\n",
    "print('Python version: ' + sys.version)\n",
    "print('Spark version: '+ spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri manipülasyonu için data yüklemesini yapalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_location = \"movie_data_part1.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \"|\"\n",
    "\n",
    "df = spark.read.format(file_type)\\\n",
    ".option(\"inferSchema\", infer_schema)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive'dan veri okumak için aşağıdaki komut kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from database.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verimizi okuduktan sonra yapılacak ilk işlem meta verileri okumaktır. Bunun için aşağıdaki kodu kullanabiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Metadata\n",
    "\n",
    "Metadata bilgilerini df.printSchema() komutu ile okuyabiliriz. Bunun yanısıra df.dtypes komutuylada aynı bilgileri alabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belongs_to_collection', 'string'),\n",
       " ('budget', 'string'),\n",
       " ('id', 'string'),\n",
       " ('original_language', 'string'),\n",
       " ('original_title', 'string'),\n",
       " ('overview', 'string'),\n",
       " ('popularity', 'string'),\n",
       " ('production_companies', 'string'),\n",
       " ('production_countries', 'string'),\n",
       " ('release_date', 'string'),\n",
       " ('revenue', 'string'),\n",
       " ('runtime', 'string'),\n",
       " ('status', 'string'),\n",
       " ('tagline', 'string'),\n",
       " ('title', 'string'),\n",
       " ('vote_average', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri tiplerini göstermeden kolonları df.columns komutuyla gösterebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belongs_to_collection',\n",
       " 'budget',\n",
       " 'id',\n",
       " 'original_language',\n",
       " 'original_title',\n",
       " 'overview',\n",
       " 'popularity',\n",
       " 'production_companies',\n",
       " 'production_countries',\n",
       " 'release_date',\n",
       " 'revenue',\n",
       " 'runtime',\n",
       " 'status',\n",
       " 'tagline',\n",
       " 'title',\n",
       " 'vote_average']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bir veri kümesindeki yapacağımız kontrollerden biride veri sayısıdır. Bunun için aşağıdaki komutu kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam kayıt sayısı: 43998\n"
     ]
    }
   ],
   "source": [
    "print('Toplam kayıt sayısı: ' + str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sütünları Bölerek İncelemek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İncelemek istediğimiz sütunları tanımlıyoruz.\n",
    "select_columns = ['id', 'budget', 'popularity','release_date','revenue','title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli sütunlarla dataframeden alt küme oluşturma.\n",
    "df = df.select(*select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "|   id| budget|        popularity|release_date|revenue|               title|\n",
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "|43000|      0|             2.503|  1962-05-23|      0|The Elusive Corporal|\n",
      "|43001|      0|              5.51|  1962-11-12|      0|  Sundays and Cybele|\n",
      "|43002|      0|              5.62|  1962-05-24|      0|Lonely Are the Brave|\n",
      "|43003|      0|             7.159|  1975-03-12|      0|          F for Fake|\n",
      "|43004| 500000|             3.988|  1962-10-09|      0|Long Day's Journe...|\n",
      "|43006|      0|             3.194|  1962-03-09|      0|           My Geisha|\n",
      "|43007|      0|             2.689|  1962-10-31|      0|Period of Adjustment|\n",
      "|43008|      0|             6.537|  1959-03-13|      0|    The Hanging Tree|\n",
      "|43010|      0|             4.297|  1962-01-01|      0|Sherlock Holmes a...|\n",
      "|43011|      0|             4.417|  1962-01-01|      0|  Sodom and Gomorrah|\n",
      "|43012|7000000|4.7219999999999995|  1962-11-21|4000000|         Taras Bulba|\n",
      "|43013|      0|             2.543|  1962-04-17|      0|The Counterfeit T...|\n",
      "|43014|      0|             4.303|  1962-10-24|      0|     Tower of London|\n",
      "|43015|      0|             3.493|  1962-12-07|      0|Varan the Unbelie...|\n",
      "|43016|      0|             2.851|  1962-01-01|      0|Waltz of the Tore...|\n",
      "|43017|      0|             4.047|  1961-10-11|      0|         Back Street|\n",
      "|43018|      0|             2.661|  1961-06-02|      0|Gidget Goes Hawaiian|\n",
      "|43019|      0|             3.225|  2010-05-28|      0|Schuks Tshabalala...|\n",
      "|43020|      0|              5.72|  1961-06-15|      0|The Colossus of R...|\n",
      "|43021|      0|             3.292|  2008-08-22|      0|          Sex Galaxy|\n",
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aşağıdaki komutla veriyi gösterelim.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tüm işlemler aşağıdaki gibi tek bir satırda ifade edilebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "|   id| budget|        popularity|release_date|revenue|               title|\n",
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "|43000|      0|             2.503|  1962-05-23|      0|The Elusive Corporal|\n",
      "|43001|      0|              5.51|  1962-11-12|      0|  Sundays and Cybele|\n",
      "|43002|      0|              5.62|  1962-05-24|      0|Lonely Are the Brave|\n",
      "|43003|      0|             7.159|  1975-03-12|      0|          F for Fake|\n",
      "|43004| 500000|             3.988|  1962-10-09|      0|Long Day's Journe...|\n",
      "|43006|      0|             3.194|  1962-03-09|      0|           My Geisha|\n",
      "|43007|      0|             2.689|  1962-10-31|      0|Period of Adjustment|\n",
      "|43008|      0|             6.537|  1959-03-13|      0|    The Hanging Tree|\n",
      "|43010|      0|             4.297|  1962-01-01|      0|Sherlock Holmes a...|\n",
      "|43011|      0|             4.417|  1962-01-01|      0|  Sodom and Gomorrah|\n",
      "|43012|7000000|4.7219999999999995|  1962-11-21|4000000|         Taras Bulba|\n",
      "|43013|      0|             2.543|  1962-04-17|      0|The Counterfeit T...|\n",
      "|43014|      0|             4.303|  1962-10-24|      0|     Tower of London|\n",
      "|43015|      0|             3.493|  1962-12-07|      0|Varan the Unbelie...|\n",
      "|43016|      0|             2.851|  1962-01-01|      0|Waltz of the Tore...|\n",
      "|43017|      0|             4.047|  1961-10-11|      0|         Back Street|\n",
      "|43018|      0|             2.661|  1961-06-02|      0|Gidget Goes Hawaiian|\n",
      "|43019|      0|             3.225|  2010-05-28|      0|Schuks Tshabalala...|\n",
      "|43020|      0|              5.72|  1961-06-15|      0|The Colossus of R...|\n",
      "|43021|      0|             3.292|  2008-08-22|      0|          Sex Galaxy|\n",
      "+-----+-------+------------------+------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id','budget','popularity','release_date','revenue','title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------------+-------+--------------------+\n",
      "|   id|budget|popularity|release_date|revenue|               title|\n",
      "+-----+------+----------+------------+-------+--------------------+\n",
      "|43000|     0|     2.503|  1962-05-23|      0|The Elusive Corporal|\n",
      "|43001|     0|      5.51|  1962-11-12|      0|  Sundays and Cybele|\n",
      "|43002|     0|      5.62|  1962-05-24|      0|Lonely Are the Brave|\n",
      "|43003|     0|     7.159|  1975-03-12|      0|          F for Fake|\n",
      "|43004|500000|     3.988|  1962-10-09|      0|Long Day's Journe...|\n",
      "+-----+------+----------+------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gösterilecek satırı sınırlayabiliriz.\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksik değerleri bir sütun için veya birden çok sütun için hesaplaybiliriz. Üç farklı şekilde null değerleri sayabiliriz.\n",
    "Buradaki komutlarda ilk olarak dataframe de bir filtre işlemi yapıyoruz. Sonrasında \"|\" işareti veya OR ifadesiyle şart belirliyoruz. İlk sütunda popülerlik sütununda bulunan boş ifadeleri ariyoruz. İkinci durumda null dolduğunda true değeri döndüren .isNull() operatörünü kullanırız. Üçüncü durumda NaN ifadelerini yakalamak için isnan kullandık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.filter((df['popularity'] == '')| df['popularity'].isNull()|isnan(df['popularity'])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğer dataframe içindeki tüm eksik değerleri hesaplamak istersek aşağıdaki komutu kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------------+-------+-----+\n",
      "| id|budget|popularity|release_date|revenue|title|\n",
      "+---+------+----------+------------+-------+-----+\n",
      "|125|   125|       215|         221|    215|  304|\n",
      "+---+------+----------+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when((col(c)== '')| col(c).isNull()| isnan(c),c)).alias(c) \n",
    "           for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu komutla tüm sütunları seçeriyoruz ve önceki eksik kontrolleri bir döngüde çalıştırıyoruz. Sonrasında eksik değerleri alt gruplar halinde gösteriyoruz.\n",
    "Dikkat etmemiz gereken nokta pyspark.sql.functions içeri aktardığımızdan emin olalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Way Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kategorik değişkenlerin frekanslarını nasıl hesaplayabileceğimize bakalım. Öncelikle veri setimiz içinde tekrar eden başlıklar olup olmadığını aşağıdaki komutla kontrol edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|count|\n",
      "+--------------------+-----+\n",
      "|   The Corn Is Green|    1|\n",
      "|Meet The Browns -...|    1|\n",
      "|Morenita, El Esca...|    1|\n",
      "| Father Takes a Wife|    1|\n",
      "|The Werewolf of W...|    1|\n",
      "|My Wife Is a Gang...|    1|\n",
      "|Depeche Mode: Tou...|    1|\n",
      "|  A Woman Is a Woman|    1|\n",
      "|History Is Made a...|    1|\n",
      "|      Colombian Love|    1|\n",
      "|        Ace Attorney|    1|\n",
      "|     Not Like Others|    1|\n",
      "|40 Guns to Apache...|    1|\n",
      "|          Middle Men|    1|\n",
      "|         It's a Gift|    1|\n",
      "|    La Vie de Bohème|    1|\n",
      "|Rasputin: The Mad...|    1|\n",
      "|The Ballad of Jac...|    1|\n",
      "|         How to Deal|    1|\n",
      "|             Freaked|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df['title']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukardaki komut veri setindeki bir başlığın kaç defa tekrarlandığını göstermektedir. Sıralı şekilde görmek için aşağıdaki komutu kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|title               |count|\n",
      "+--------------------+-----+\n",
      "|null                |304  |\n",
      "|Les Misérables      |8    |\n",
      "|The Three Musketeers|8    |\n",
      "|Cinderella          |8    |\n",
      "|The Island          |7    |\n",
      "|A Christmas Carol   |7    |\n",
      "|Hamlet              |7    |\n",
      "|Dracula             |7    |\n",
      "|Frankenstein        |7    |\n",
      "|Crime and Punishment|6    |\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df['title']).count().sort(desc('count')).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada tekrarlalan bir çok başlık olduğunu görüyoruz. Eksik başlık değerleri esas sorunumuz olarak görülmektedir. Gerçek dünya verileri de bu şekilde karmaşık olabilir. Burada bir ayarlama yapalım. Eksik değerleri ortadan kaldırarak ve yalnızca 4 keredan fazla tekrarlanan başlıkları filtreleyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksik değerleri ortadan kaldırmak ve geçici bir dataframe alt küme oluşturma\n",
    "df_temp = df.filter((df['title'] != '') & \n",
    "                    (df['title'].isNotNull())& (~isnan(df['title'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|title               |count|\n",
      "+--------------------+-----+\n",
      "|Les Misérables      |8    |\n",
      "|The Three Musketeers|8    |\n",
      "|Cinderella          |8    |\n",
      "|The Island          |7    |\n",
      "|Hamlet              |7    |\n",
      "|A Christmas Carol   |7    |\n",
      "|Dracula             |7    |\n",
      "|Frankenstein        |7    |\n",
      "|Beauty and the Beast|6    |\n",
      "|Crime and Punishment|6    |\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame de 4 defadan fazla tekrarlanan değerleri filtreleyerek dataframe oluşturma\n",
    "df_temp.groupby(df_temp['title']).count().filter(\"count > 4\").sort(col(\"count\").desc()).show(10,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azalan işlevini belirtmezsek sonuçlar varsayılan olarak artan sırada sıralanır. Aşağıdaki komutla 4 ve daha fazla tekrarlanan başlıkların sayısını bulmaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(df_temp['title']).count().filter(\"count >4\").sort(col(\"count\").desc()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gecici olarak oluşturduğumuz dataframe yi aşağıdaki komutla silebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta verilerden gözlemlediğimiz tüm değişkenler diziler olarak listelenir. Verileri inceledikten sonra bu alanlardan bazıları tamsayı, float veya tarih nesneleri olduğunu görüyoruz. Veri türleri doğru tanımlanmazsa bazı işlemler yanıltıcı sonuçlara yol açabilir. Analizlerimizde doğru veri türünü belirleme konusunda gerekli özeni veri türünü belirleme konusunda gerekli özeni göstermemiz gerekiyor. Şimdi değişlenlerin doğru veri türüne nasıl dönüştürüleceğini görelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('budget', 'string'),\n",
       " ('popularity', 'string'),\n",
       " ('release_date', 'string'),\n",
       " ('revenue', 'string'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before Casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri setindeki tüm alanların veri türü string olarak görülüyor. Burada analizlere başlamadan önce doğru veri dönüşümlerini yapmamız gerekmektedir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting \n",
    "df = df.withColumn('budget', df['budget'].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('budget', 'float'),\n",
       " ('popularity', 'string'),\n",
       " ('release_date', 'string'),\n",
       " ('revenue', 'string'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After Casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast fonksiyonunu kullandık. Ayrıca ek olarak burada .withColumn kullandık. Bu komut PySpark'ta kullanılan en yaygın fonksiyonlardan biridir. Bu fonksiyon veri türlerinin değerlerini güncellemek, yeniden adlandırmak ve veri dönüşümü için kullanılmaktadır. Burada bu fonksiyonu birden çok sütuna nasıl genişletebileceğimizi aşağıda gösteriyoruz. Veri türlerini bildiğimizi varsayalım. Bu durumda dönüştürülecek değişkenler listesi oluşturabilir ve bunları döngüde kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying and assignig lists of variables\n",
    "int_vars = ['id']\n",
    "float_vars = ['budget','popularity', 'revenue']\n",
    "date_vars = ['release_date']\n",
    "\n",
    "# Converting integer variables\n",
    "for column in int_vars:\n",
    "     df=df.withColumn(column,df[column].cast(IntegerType()))\n",
    "        \n",
    "for column in float_vars:\n",
    "     df=df.withColumn(column,df[column].cast(FloatType()))\n",
    "        \n",
    "for column in date_vars:\n",
    "     df=df.withColumn(column,df[column].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('budget', 'float'),\n",
       " ('popularity', 'float'),\n",
       " ('release_date', 'date'),\n",
       " ('revenue', 'float'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data types of all variables aftes casting\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|id   |budget  |popularity|release_date|revenue|title                                  |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|43000|0.0     |2.503     |1962-05-23  |0.0    |The Elusive Corporal                   |\n",
      "|43001|0.0     |5.51      |1962-11-12  |0.0    |Sundays and Cybele                     |\n",
      "|43002|0.0     |5.62      |1962-05-24  |0.0    |Lonely Are the Brave                   |\n",
      "|43003|0.0     |7.159     |1975-03-12  |0.0    |F for Fake                             |\n",
      "|43004|500000.0|3.988     |1962-10-09  |0.0    |Long Day's Journey Into Night          |\n",
      "|43006|0.0     |3.194     |1962-03-09  |0.0    |My Geisha                              |\n",
      "|43007|0.0     |2.689     |1962-10-31  |0.0    |Period of Adjustment                   |\n",
      "|43008|0.0     |6.537     |1959-03-13  |0.0    |The Hanging Tree                       |\n",
      "|43010|0.0     |4.297     |1962-01-01  |0.0    |Sherlock Holmes and the Deadly Necklace|\n",
      "|43011|0.0     |4.417     |1962-01-01  |0.0    |Sodom and Gomorrah                     |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veriyi analiz etmek için veri türü, dağılımı ve dağılım hakkında detaylı bilgilere sahip olmamız gerekiyor. Spark'ta hızlı bir şekilde bunu yapabileceğimiz 'descripe' fonksiyonu bulunmaktadır. Spark'taki bu fonksiyon her sütun için ortalama, standart sapma ve minimum ve maksimum değerler için  eksik olmayan değerlerin sayısını verdiğinden çok kullanışlı bir fonksiyondur. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-----------------+-------------------+--------------------+\n",
      "|summary|                id|              budget|       popularity|            revenue|               title|\n",
      "+-------+------------------+--------------------+-----------------+-------------------+--------------------+\n",
      "|  count|             43784|               43873|            43783|              43783|               43694|\n",
      "|   mean|44502.304312077475|   3736901.834963167|5.295444259579189|  9697079.597382545|            Infinity|\n",
      "| stddev|27189.646588626394|1.5871814952777326E7|6.168030519208252|5.687938449628811E7|                 NaN|\n",
      "|    min|                 2|                 0.0|              0.6|                0.0|!Women Art Revolu...|\n",
      "|    max|            100988|               3.8E8|            180.0|       2.78796518E9|       시크릿 Secret|\n",
      "+-------+------------------+--------------------+-----------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buradaki ortalama merkezi eğilim ölçüsü olan artimetik ortalamadır. Ortalama birçok işlem için iyi bir tahmin edicidir fakat aykırı değerlerden etkilenmektedir. Aykırı değerler ortalama üzerinde olumsuz etkilere sahiptir. Median (ortanca) aykırı değerlerden etkilenmediği için daha iyi bir tahmin edicidir ancak hesaplanma maliyeti yüksektir. Spark'ta da bu işlemin maliyeti yüksektir. Sıralama işlemi yapılacağı için yinede hesaplama maliyeti açısından olumsuz bir durum oluşmaktadır. Spark bu işlemi daha hızlı yapmak için Greenwald-Khanna algoritmasının bir varyantını kullanmaktadır. Medyanı hesaplamak için kullanılan işlev approxQuantile'dir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medyan hesaplanırken verilmesi gereken 3 parametre vardır. Bu parametreleri'.approxQuantile' fonksiyonunda girmemiz gerekmektedir. <p>\n",
    "'col': Sayılsal sütun adı<p>\n",
    "'probabilities': Olasılık listesi. Burada vereceğimiz değer 0-1 arasında olmalıdır. Örneğin, 0 minimum, 0.5 medyan, 1 maksimumdur. <p>\n",
    "'relativeError': Hesaplamanın kesinliği ayarlanmaktadır (>=0). 0 olarak ayarlanmışsa, kesin bir hesaplama yapılır ve hesaplama maliyeti yüksektir. 1'den büyük değerlerin kabul edilmektedir ama 1 ile aynı sonucu vermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'budget' sütunu için medya hesaplamayı deneyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Öncelikle 'budget' bilinmeyen değerleri 0 olarak işaretlendiğinden,\n",
    "# medyanı hesaplamadan önce bu değerleri filtreleyelim.\n",
    "df_temp = df.filter((df['budget'] != 0) & (df['budget'].isNotNull()) & (~isnan(df['budget'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burada ikinci parametre 0.5 değeri medyan değerini gösterir.\n",
    "\n",
    "median = df_temp.approxQuantile('budget', [0.5], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median of budget is [5000000.0]\n"
     ]
    }
   ],
   "source": [
    "print('The median of budget is ' + str(median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birden fazla sütunun medyanın aynı anda hesaplamak için listeye ilk parametreyi değiştirmemiz yeterlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique/Distinct Values and Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bazen bir değişkendeki düzeylerin sayısını öğrenmek isteyebiliriz. Spark'ta bunu countDistinct fonksiyonunu kullanarak yapabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|41138|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Farklı başlıklarını sayalım\n",
    "df.agg(countDistinct(col('title')).alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|title                                        |\n",
      "+---------------------------------------------+\n",
      "|The Corn Is Green                            |\n",
      "|Meet The Browns - The Play                   |\n",
      "|Morenita, El Escandalo                       |\n",
      "|Father Takes a Wife                          |\n",
      "|The Werewolf of Washington                   |\n",
      "|My Wife Is a Gangster                        |\n",
      "|Depeche Mode: Touring the Angel Live in Milan|\n",
      "|A Woman Is a Woman                           |\n",
      "|History Is Made at Night                     |\n",
      "|Colombian Love                               |\n",
      "+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Farklı fonksiyonları kullanarak başlık sayımını yapabiliriz\n",
    "df.select('title').distinct().show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yıllara göre farklı başlıklara bakmak için aşağıdaki kodu kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year from release date\n",
    "df_temp = df.withColumn('release_year', year('release_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month\n",
    "df_temp = df_temp.withColumn('release_month', month('release_date'))\n",
    "\n",
    "# Extracting day of month\n",
    "df_temp = df_temp.withColumn('release_day', dayofmonth('release_date'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|release_year|count(title)|\n",
      "+------------+------------+\n",
      "|1959        |271         |\n",
      "|1990        |496         |\n",
      "|1975        |365         |\n",
      "|1977        |415         |\n",
      "|1924        |19          |\n",
      "|2003        |1199        |\n",
      "|2007        |1896        |\n",
      "|2018        |4           |\n",
      "|1974        |434         |\n",
      "|2015        |13          |\n",
      "+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yıllara göre başlık sayısının hesaplanması\n",
    "df_temp.groupBy('release_year').agg(countDistinct('title')).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu fonksiyon Spark'taki önemli fonksiyonlardandır. SQL'de 'where' fonksiyonuyla ayndır. 'filter' standart Scala'daki adıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".where() = .filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daha önce filtre veya koşulları kullanarak verilerimizi filtreleme yapmıştık. Burada verileri filtrelemek için farklı yolları deneyelim. Örneğin, 'Meet' ile başlayan tüm başlıkları listelemeyi deneyelim. SQL kullanıyorsanız bu işlemi SQL benzer şekilde yapabilirsiniz. PySpark bu amaçla kullanılabilecek regex fonksiyonlarını bizlere sunmaktadır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "|id   |budget   |popularity|release_date|revenue    |title                     |\n",
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "|43957|500000.0 |2.649     |2005-06-28  |1000000.0  |Meet The Browns - The Play|\n",
      "|39997|0.0      |3.585     |1989-11-15  |0.0        |Meet the Hollowheads      |\n",
      "|16710|0.0      |11.495    |2008-03-21  |4.1939392E7|Meet the Browns           |\n",
      "|20430|0.0      |3.614     |2004-01-29  |0.0        |Meet Market               |\n",
      "|76435|0.0      |1.775     |2011-03-31  |0.0        |Meet the In-Laws          |\n",
      "|76516|5000000.0|4.05      |1990-11-08  |485772.0   |Meet the Applegates       |\n",
      "|7278 |3.0E7    |11.116    |2008-01-24  |8.4646832E7|Meet the Spartans         |\n",
      "|32574|0.0      |7.42      |1941-03-14  |0.0        |Meet John Doe             |\n",
      "|40506|0.0      |4.814     |1997-01-31  |0.0        |Meet Wally Sparks         |\n",
      "|40688|2.4E7    |6.848     |1998-03-27  |4562146.0  |Meet the Deedles          |\n",
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['title'].like('Meet%')).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi 's' ile bitmeyen başlıkları bulalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------------+------------+------------------------+\n",
      "|id   |budget|popularity|release_date|revenue     |title                   |\n",
      "+-----+------+----------+------------+------------+------------------------+\n",
      "|43100|0.0   |7.252     |1959-10-07  |0.0         |General Della Rovere    |\n",
      "|43152|0.0   |5.126     |2001-06-21  |0.0         |Love on a Diet          |\n",
      "|43191|0.0   |4.921     |1952-08-29  |0.0         |Beware, My Lovely       |\n",
      "|43281|0.0   |2.411     |1989-11-22  |0.0         |Love Without Pity       |\n",
      "|43343|0.0   |3.174     |1953-12-25  |0.0         |Easy to Love            |\n",
      "|43347|3.0E7 |14.863    |2010-11-22  |1.02820008E8|Love & Other Drugs      |\n",
      "|43362|0.0   |1.705     |1952-02-23  |0.0         |Love Is Better Than Ever|\n",
      "|43363|0.0   |2.02      |1952-05-29  |0.0         |Lovely to Look At       |\n",
      "|43395|0.0   |4.758     |1950-11-10  |0.0         |Two Weeks with Love     |\n",
      "|43455|0.0   |4.669     |1948-08-23  |0.0         |The Loves of Carmen     |\n",
      "+-----+------+----------+------------+------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# İçerisinde 'ove' ifadesini geçtiği başlıkları bulalım.\n",
    "\n",
    "df.filter(df['title'].rlike('\\w*ove')).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+-----------+--------------------------------+\n",
      "|id   |budget  |popularity|release_date|revenue    |title                           |\n",
      "+-----+--------+----------+------------+-----------+--------------------------------+\n",
      "|43363|0.0     |2.02      |1952-05-29  |0.0        |Lovely to Look At               |\n",
      "|43400|0.0     |1.254     |2008-01-01  |0.0        |Pressure Cooker                 |\n",
      "|43796|0.0     |3.486     |1941-10-24  |0.0        |Spooks Run Wild                 |\n",
      "|43822|0.0     |3.87      |1940-01-22  |0.0        |The Stars Look Down             |\n",
      "|21284|0.0     |1.36      |1994-10-07  |0.0        |The Snooks in the Limelight     |\n",
      "|21358|0.0     |6.312     |2009-08-29  |0.0        |Merlin and the Book of Beasts   |\n",
      "|21509|210000.0|8.035     |2008-04-01  |0.0        |The Cook                        |\n",
      "|21845|0.0     |9.402     |1993-07-07  |5.3579268E7|Rookie of the Year              |\n",
      "|21874|0.0     |6.491     |1981-10-30  |3282232.0  |Looker                          |\n",
      "|31173|0.0     |1.4       |1969-12-18  |0.0        |Ludwig on the Lookout for a Wife|\n",
      "+-----+--------+----------+------------+-----------+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['title'].rlike('\\w*ook')).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada örnekleri kendiniz çoğaltabilirsiniz. Şimdi regex ile ilgili çok sık kullanacağınız kısa bilgileri paylaşalım. <p>\n",
    "Regex de kullanılan meta karakteriler aşağıdaki gibidir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tek karakterler: \n",
    "<li> \\d: 0-9 arasındaki sayıları tanımlar </li>\n",
    "<li> \\w: 0'dan 9'a kadar sayırları ve tüm büyük ve küçük harfleri tanımlar </li>\n",
    "<li> \\s: Boşluk </li>\n",
    "<li> .(nokta): Herhangi bir karakter </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayrıca kaç karakterin aranacağını ayarlayabileceğimiz ifadeleri kullanabiliriz.\n",
    "<li> *: 0 veya daha fazla karakter\n",
    "<li> +: 1 veya daha fazla karakter\n",
    "<li> ?: 0 veya 1 karakter \n",
    "<li> {min,max}: min ve max değerleri dahil aralık arasındaki tüm karakterleri\n",
    "<li> {n}: Tam olarak n karakter  "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAALcAAAA3CAYAAABO3ttHAAAMMklEQVR4Ae1cDVBU1R5noWWxxiRLR+WpL1PKSk0xzMyXaJIC4gMCnp8oCwaI9hKtlyg+RfxCyq8klRCSBEVBQMRRiB4CaTaC8SUjscKIKKOLo8vAzt7b783iwt7dvXu5F2iVvXdn7nDO/5zz//yd/zn33HuxgPATPGCmHrAwU7sEswQPQAC3AAKz9YAAbrMNrWCYAG4BA2brAQHcZhtawTAB3AIGzNYDArjNNrSCYQK4BQyYrQcEcJttaAXDBHALGDBbDwjgNtvQCoYJ4BYwYLYeEMBttqEVDBPALWBAxwMXL17UqffligDuvhw9Jt2VDbickYxjP55D5UOmjtq2yspKhIWFaQl9vCSAu48HkF59JX4Oc0bo+YdoPu0HhxXn0ErfUYe6detWFBUV6dD6csX8wE3eR2nOKaSmpnK6TuWU4j5JF0oSLYoWuoZnhtbaog9dEg9vydrtUWT64w3fZLBJ3q6urvjzzz+fGbt6qsizAe5uLKFGDSflyP18HPqJLGAheh72c5YjICDAyCXF0k9mY9wgMayGSZHdps9VhZqTm7EjuwG0uNfv3sv19PR0+Pn50V7r16/XSFOhPGEj9hU2G0gnHxQgyi8YCVVKgzZ9gkwmQ0hIiD65T9efAXB3bwll9LqyAgfmDIalhQjPTwhDnpwZmsSdVCwd64UkhS5XZclOBGwpxtPK2yUlJTh69CjtdebMGa2yhAxxK0KQcptiZ/NlJMYk4WozidbaGtwmtN3pSjExMdC5mSQbkffdAZyv62IgHbMe0lRlyYhJLuux358BcHdvCe3Kf+S9swgeK4FIZAU7j3jUMMZIhetRYfi2ngIOsh7xS6VIvEehdSX0Kba3FX+Jj1dk4Un+bkPB2g/hONcd7u4u+MAzGhWM9gPu7u5QqVQaC1pwZWcwdl5+WtOaQG3CSqw8JkMXajN63HTgJu6i+MgXCAxYhdUhy+DrLcXGFO3s5LKEMlpEaWy9HoOZL1vCQtQfk8MvMe47ibpKVFE2pi35q+HglYj7nfxINOTuwdrgQAQEBCJk7V7k3SEBog45MWsREhiIkHX7kX+3NyZDN2SpruArR2fs/YM7HBobG+Hv799paVtJFDxXZuJRJ+UvKpBtkN8qQ/GFNOSUynW3fkQldrh54cgt7vZ0aGsicKtQEjkFQ512o6J9X0ugMW057GdEoz0WHJfQDuW7/kviTpoUY6xFEIlHYsHxOpaZQIHM5SMwddsNvf4kmnJW4k2JGJMiStCxk1UUhGH6nGj81qto4CqrFWelIzF1W6Wezl17KTY2FpmZmU86knX4zvUtfHpe/ya1az7cehCQZW7D6sXvY5iVGJM2/Y6OdUOjCOr2zcbbq/JZnfTQyTYNuFXXsWlSP0zYWKI1QFWKbyISUU9yX0LpDDFOa8HVqOmwtbSApe00RF7R21jTDWzLw8pRQ7Esky7Aj/HzmjchsXXC11UqoOUKNnsHI02dxXv9x0UWibpvZqD/9BjUckx2np6eaGt7cjdN3NiJaS/PR8KDXjeGnmFLCnxepAM3oPptA96xW4zTlBWVngk91TTgJqoQ5SiGtb0/Umqewj6OrMeJxa9CLBJBPHo5TjcwA5G4sR3v9ZuAjSW6uaTThc3nETxGjIHOMUjZ5IcN+d30fidDhgIHWa2nFmKgrTeSOawgcrkcixYt0ihAouHb2ej/xjoUdyxL6haiDlkRvpjxjgNCf7yEI2u84TT+7xg53h1bfmrS3U4wmELb1HICPgPowQ3FcXgNsIM0my7J0HLTIZoG3CBx+8RSjFJvD54fCaegr5FZ2czBKY9w9fgubN++nfbalViMB8x4BR4VI2LKi7C0sMRAp90oZfBX61l/DLWZhW8bjTEl0ZS+DCOsnsNwadZfvDdlL0tZ8G+MsXHA5jIjk1In9E8qCQkJSElJ0bS0ISfQDjZO+2Aw/xVqEFrB9v1w5N1RAspq7P/YFhLHKFRyXCl01GACt/JnrBplA8eoKp0hbCsmArdaHRJNl+PxhedEDFaD3PpvmPXfPDQZw4+OBa2Q/ZKNrKws2iu7sAaPdfrTV4jaBHjaiTFsfhxuMsS/+XtXSCRuiGfIgGT99/AcKoHVEE8k1rEyQp0C0Xz7Jqqrq2mvm/Vy7baNYgJbWaprGzDeegSCLhoc2FO46RZ9fX3x+HGH9+SIc5FA4nbUcMK2pmPJKxL8I0amSUoEKqMcIbELwDmqOKIZt2/S21ddfRP1cj3HM4FbdQ0bJlhj+KcXdJVmWTMhuLUaKapOInTyAFiKx2H9VT1jtd16vdSc/wWmOn6GC/SPIjXySDTsnwnrfh5IMraDIutwTLoAMbnx8BwixlDvJFCPmI0rLkdedJCRB0oBCIrKhsFiwUEWUb4Fk60Hwy+DYVmiKKdQKODj46OlkA3Y52SNfh7HDM+YNeB22lvfCe6aXdMg0X/4Jc9DdJCxh2ZBiMpu1MpTl5jATVQgcrI1Bvll6I5hWTMNuFtzELklV8dhqqvrMU5si3+lGkMQxQKyCenrXOHs7Ex7uaxOBvWImjKys0jIjsF34hzsKaOmmc5mncK9WGdYS+bhKG3mJiCLXwSPHb9DCQKyuHkYJLbDghN3OGyzdMQxVLjJUpVvgYP1MEgNH7XSyjh58mT7AyJtoyZzu8Rpzsu1LWALbsoQVkUmcKuuI2Ki5BnP3G3nsGK8O47ItJuz1vxVGN1/OqJvaGmsnNGdTooriJzhgID0RlYAVCR/gv4SZ8TeMxSmrNqP+TPD8UtHciSqse8jW4hHLMGpjrSrKkdCxGEYux815EpPYSWLMlSdMN6W2GNNIfVukNJBr7hkyRI8eEA9FtHsuT/8Grf0d1qtaVj8sgQz9lAy9873IRkqxdkOX+jxZ1VtSYH3i2JMjCg13JIpC7HG3gbvRlawYqXfyTSZm5DhuPQdDH/bE+EHf8Cx2I3weW8KFh+6rpPN9ZXrlTp5B+lSBzht/RUsDgHbRSqL1+J1G0dE6dwpPURRbDBmjbTBK1P8sP3srfbz5LZrCfj841fxnMgStuO9sDaxBEpSjrJLpSzvJ+is5CCLMrwtNxgj+zlhL4s9kvroz8PDgzJaXdSclrwehiLq/FCV4fi6OXhVbIVBU6U4WCRHU8F+LHt3ICytR8Mt/JQeHzZVAn9k7sQ66YewsxLhhbHzERoeh2LqwVPLCfjaDsGyDBarO41I04BbI1gpr0PFlUsouFyBxp7MdhpD6EltKNs7F5MWJKHrB10k7lVp7sqbk+D50mtYld/1FoZObmvlIXwyJQQ53RtOx5IV7dEP/0T/UavARm31zfnBgwcN+BLVu/DBwHmI1z6aNehjKoL6HuLdwV5Ioi4uHISbFNwc9OqFriQeXPwc7037D/5n+MKcIX+iHDuCtj2hkzLscXoJcw/T7EsMRxpSlEVY5xSke4pg2KuXKSpc3+SAIQtOMr5m0CFU/aak+rG7wY+sxyHXsQjI7l62NODXA0LT9+6w9+/+awBmC27ij3h4TZqPQzdYnMao7qJg60d4zSdZEwoCtXtn4Y3gi+hW8n0q4G7GDx6jsDC165lMEET7i1LGcNdS+CVmLk9FNxOmMbbc6OpJtmQh4ij3adwYwEz/P7f6gY3jAAywn465Li5wMXrNxaypb2G4rRgikRjjwq92+o9sSsbCaWG4RN17drYyFx6XH8C80bOw7ZqcuWNvtj48Df/ZG/Eri9mYl5eH3bt3M0h/jKLIZdiQa0L9dbQhcT97A1bH3eT8ngyVjfllbvIu0paPglj9sYIFh0v0AuYnULOeCuXR3gg8cY/VCQvVqaYvk6iPD8JnafdZ6RoaGora2lpmNclG5O6PQVYPMiezAOOtqsoMHM6oYH0AYIyT+YEbj1D7WxEKCwu5XUW/okb/owZlJQ6v/gpn/pKXooyFhDudqMtAVNSZLs/61ZzVn5G5ublxF9IHR5ghuHs5Cg+vIeNCTY+Wx17WSI+dEr/n/oR6lo8L1B8kqD8p48NPADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTGwVw8zTwfDBbADcfosxTG/8PAz+1W7hikE8AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Columns\n",
    "Yeni sütunlar oluşturmak için Spark'ta birçok yöntem kullanılmaktadır. En basit yöntem withColumn fonksiyonudur. 'popularity' değişkeni için varyans hesaplamak istediğimizi varsayalım. Varyans, verilerin ortalamadan ne kadar uzağa yayıldığının bir ölçüsüdür. Ortalamayı hesaplayarak varyans hesaplamasına başlayalım. Varyans formülü:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pop=df.agg({'popularity': 'mean'}).collect()[0]['avg(popularity)']\n",
    "count_obs= df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varyans formülünden anlaşılacağı gibi her bir gözlemden ortalamayı çıkarmamız gerekiyor. Bu işlemi \"withColumn\" ve \"lit\" fonksiyonunu kullanarak yapabiliriz. Lit fonksiyonu, sütuna göre değişmeyen değerlerle işlem yapmamızı sağlıyor. DataFrame doğrudan sabit değer ekleme için kullanılan bir fonksiyondur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('mean_popularity', lit(mean_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+-------+--------------------+-----------------+\n",
      "|   id|  budget|popularity|release_date|revenue|               title|  mean_popularity|\n",
      "+-----+--------+----------+------------+-------+--------------------+-----------------+\n",
      "|43000|     0.0|     2.503|  1962-05-23|    0.0|The Elusive Corporal|5.295444259579189|\n",
      "|43001|     0.0|      5.51|  1962-11-12|    0.0|  Sundays and Cybele|5.295444259579189|\n",
      "|43002|     0.0|      5.62|  1962-05-24|    0.0|Lonely Are the Brave|5.295444259579189|\n",
      "|43003|     0.0|     7.159|  1975-03-12|    0.0|          F for Fake|5.295444259579189|\n",
      "|43004|500000.0|     3.988|  1962-10-09|    0.0|Long Day's Journe...|5.295444259579189|\n",
      "+-----+--------+----------+------------+-------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varyans formülündeki kare alma işlemi gerçekleştirmemiz gerekiyor. Bunun için PySpark'ta \"pow\" fonksiyonunu kullanabiliriz. \"pow\" fonksiyonu sayının belirttiğimiz kuvvetini almamızı sağlamaktadır.  Formülden de anlaşılacağı üzere burada kare almamız gerekiyor bunun için pow fonksiyonuna parametre olarak 2 değerini veriyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('varaince',pow((df['popularity']-df['mean_popularity']),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_sum = df.agg({'varaince': 'sum'}).collect()[0]['sum(varaince)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.858688057662825"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varaince_population = variance_sum/(count_obs-1)\n",
    "varaince_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konunun pekişmesi açısından hem 'budget' hemde 'popularity' değişkenleri için yüksek, orta ve düşük olarak değerleri sınıflandırmak istediğimizi varsayalım. İlk yapacağımız iş bu etiketler için eşik değerlerinin berlirlemektir. Bir Python fonksiyonu yazarak bu işlemi gerçekleştirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cols(budget,popularity):\n",
    "    if budget < 10000000: \n",
    "        budget_cat = 'Small'\n",
    "    elif budget < 100000000: \n",
    "        budget_cat = 'Medium'\n",
    "    else:\n",
    "        budget_cat = 'Big'\n",
    "        \n",
    "    if popularity < 3: \n",
    "        ratings = 'Low'\n",
    "    elif popularity < 5: \n",
    "        ratings = 'Mid'\n",
    "    else:\n",
    "        ratings = 'High'\n",
    "    return budget_cat, ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark'ta yeni bir kolon oluştururken yeni oluşturuan kolonların değişken tiplerini belirleyebiliyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StructType nesneleri Spark DataFrames şemasını tanımlamak için kullanılmaktadır. StructType nesneleri bir DataFrame'deki her sütun adı, türü, null değer olabilme gibi seçenekleri tanımlamamıza imkan veren bir StructField nesnesini içermektedir. \n",
    "Birden çok değer döndürmemiz gereken durumlarda StructType yapısını kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek uygulama\n",
    "udfB = udf(new_cols, StructType([StructField('budget_cat', StringType(), True),StructField('ratings', StringType(),True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.select('id','budget','popularity').withColumn('newcat',udfB('budget','popularity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+----------+-------+\n",
      "|id   |budget   |popularity|budget_cat|ratings|\n",
      "+-----+---------+----------+----------+-------+\n",
      "|43000|0.0      |2.503     |Small     |Low    |\n",
      "|43001|0.0      |5.51      |Small     |High   |\n",
      "|43002|0.0      |5.62      |Small     |High   |\n",
      "|43003|0.0      |7.159     |Small     |High   |\n",
      "|43004|500000.0 |3.988     |Small     |Mid    |\n",
      "|43006|0.0      |3.194     |Small     |Mid    |\n",
      "|43007|0.0      |2.689     |Small     |Low    |\n",
      "|43008|0.0      |6.537     |Small     |High   |\n",
      "|43010|0.0      |4.297     |Small     |Mid    |\n",
      "|43011|0.0      |4.417     |Small     |Mid    |\n",
      "|43012|7000000.0|4.722     |Small     |Mid    |\n",
      "|43013|0.0      |2.543     |Small     |Low    |\n",
      "|43014|0.0      |4.303     |Small     |Mid    |\n",
      "|43015|0.0      |3.493     |Small     |Mid    |\n",
      "|43016|0.0      |2.851     |Small     |Low    |\n",
      "+-----+---------+----------+----------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unbundle the struct type columns into individual columns and drop the struct type\n",
    "df_with_newcols = temp_df.select('id','budget','popularity','newcat').withColumn('budget_cat', temp_df.newcat.getItem('budget_cat')).withColumn('ratings', temp_df.newcat.getItem('ratings')).drop('newcat')\n",
    "df_with_newcols.show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- budget: float (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- newcat: struct (nullable = true)\n",
      " |    |-- budget_cat: string (nullable = true)\n",
      " |    |-- ratings: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  New Columns - Observe Metadata\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu işlemlerin aynısını when fonksiyonuyla da yapabiliriz. Bu fonksiyonu kullanmanın en önemli avantajı çıktı veri türünün tanımlamaya gerek olmamasıdır. Bu fonksiyon hızlı ve karmaşık işlemler için kullanılmaktadır. When fonksiyonunu kullanarak önceki işlemleri yeniden yapmaya çalışalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  New Columns - Method 3\n",
    "df_with_newcols = df.select('id','budget','popularity').\\\n",
    "withColumn('budget_cat', when(df['budget']<10000000,'Small').when(df['budget']<100000000,'Medium').otherwise('Big')).\\\n",
    "withColumn('ratings', when(df['popularity']<3,'Low').when(df['popularity']<5,'Mid').otherwise('High'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting and Renaming Columns\n",
    "Drop fonksiyonunu kullanarak her zaman herhangi bir sütunu silebiliriz. Sonucu \"printSchema\" fonksiyonuyla görebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['budget_cat']\n",
    "df_with_newcols = df_with_newcols.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- budget: float (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- budget_cat: string (nullable = false)\n",
      " |-- ratings: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_newcols.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolonlara yeniden isim vermek için \"withColumnRenamed\" fonksiyonu kullanılmaktadır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_newcols = df_with_newcols.withColumnRenamed('id','film_id').withColumnRenamed('ratings','film_ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- film_id: integer (nullable = true)\n",
      " |-- budget: float (nullable = true)\n",
      " |-- popularity: float (nullable = true)\n",
      " |-- budget_cat: string (nullable = false)\n",
      " |-- film_ratings: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_newcols.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri manipülasyonuna giriş dersi burada tamamlandı.\n",
    "Sonraki derste veri işlemenin detaylarına değinilecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
